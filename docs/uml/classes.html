<html>
  <body>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
      <div class="mermaid">
    
        classDiagram
          class APIChatBot {
            register_callbacks()
          }
          class AlephAlpha {
            client : Client
            get_model_for(model_spec: ModelSpec) Model
          }
          class AlephAlphaModel {
            client : Client
            generate_response(messages: List[Dict]) Tuple[Any, Any, str]
          }
          class Anthropic {
            client : Anthropic
            get_model_for(model_spec: backends.ModelSpec) backends.Model
          }
          class AnthropicModel {
            client : Anthropic
            encode_image(image_path)
            encode_messages(messages)
            generate_response(messages: List[Dict]) Tuple[str, Any, str]
          }
          class Backend {
            get_model_for(model_spec: ModelSpec)* Model
          }
          class Bot {
            sio : Client
            token
            uri
            user
            message_callback(success, error_msg)
            register_callbacks()*
            request_feedback(response, action)
            run()
          }
          class Cohere {
            client : Client
            get_model_for(model_spec: backends.ModelSpec) backends.Model
          }
          class CohereModel {
            client : Client
            generate_response(messages: List[Dict]) Tuple[str, Any, str]
          }
          class ContextExceededError {
            context_size : int
            tokens_left : int
            tokens_used : int
          }
          class CustomResponseModel {
            generate_response(messages: List[Dict])* Tuple[Any, Any, str]
          }
          class DialogueGameMaster {
            current_turn : int
            messages_by_names : Dict[str, List]
            players_by_names : Dict[str, Player]
            add_assistant_message(player: Player, utterance: str)
            add_message(player: Player, utterance: str, role: str)
            add_player(player: Player)
            add_user_message(player: Player, utterance: str)
            get_players() List[Player]
            log_message_to(player: Player, message: str)
            log_message_to_self(message: str)
            log_to_self(type_: str, value: str)
            play() None
            prompt(player: Player, is_reprompt)
            setup()
          }
          class GameBenchmark {
            filter_experiment : List[str]
            game_dir : str
            instances : NoneType
            build_transcripts(results_dir: str)
            compute_scores(results_dir: str)
            create_game_master(experiment: Dict, player_models: List[backends.Model])* GameMaster
            create_game_scorer(experiment: Dict, game_instance: Dict)* GameScorer
            get_description()* str
            is_single_player() bool
            run(player_models: List[backends.Model], results_dir: str)
            setup(game_path: str, instances_name: str)
          }
          class GameInstanceGenerator {
            instances : dict
            add_experiment(experiment_name: str, dialogue_partners: List[Tuple[str, str]]) Dict
            add_game_instance(experiment: Dict, game_id)
            generate(filename)
            on_generate()*
          }
          class GameMaster {
            experiment : Dict
            player_models : Optional[List[backends.Model]]
            play()* None
            setup()*
          }
          class GameRecorder {
            interactions : dict
            log_current_turn : int
            requests : list
            log_event(from_: str, to: str, action: Dict, call: Tuple[Any, Any])
            log_key(key: str, value: Any)
            log_next_turn()
            log_players(players_dic: Dict)
            store_records(results_root: str, dialogue_pair_desc: str, game_record_dir: str)
          }
          class GameResourceLocator {
            logger : NoneType, RootLogger
            name : str
            file_path(file_name: str) str
            load_csv(file_name: str) Dict
            load_file(file_name: str, file_ending: str) str
            load_instances(game_path, instances_name)
            load_json(file_name: str) Dict
            load_results_json(file_name: str, results_dir: str, dialogue_pair: str) Dict
            load_template(file_name: str) str
            results_path_for(results_dir: str, dialogue_pair: str)
            store_file(data, file_name: str, sub_dir: str)
            store_results_file(data, file_name: str, dialogue_pair: str, sub_dir: str, root_dir: str)
          }
          class GameScorer {
            experiment : Dict
            game_instance : Dict
            scores : dict
            compute_scores(episode_interactions: Dict) None
            log_episode_score(score_name, score_value)
            log_main_score(episode_interactions: Dict)*
            log_turn_score(turn_idx, score_name, score_value)
            score_game(episode_interactions: Dict) None
            score_game_end(episode_interactions: Dict) None
            score_requests(episode_interactions: Dict)
            score_turns(episode_interactions: Dict)* None
            store_scores(results_root: str, dialogue_pair: str, game_record_dir: str)
          }
          class GameSpec {
            game_path
            from_dict(spec: Dict)
            game_file_exists()
            get_game_file()
            matches(spec: Dict)
          }
          class GenericChatbot {
            players_per_room : dict
            waiting_room : NoneType
            close_game(room_id)
            command_stop(room_id, user_id)
            confirmation_code(room_id, status, receiver_id)
            join_task_room()
            room_to_read_only(room_id)
          }
          class GenericOpenAI {
            client : OpenAI
            get_model_for(model_spec: backends.ModelSpec) backends.Model
            list_models()
          }
          class GenericOpenAIModel {
            client : OpenAI
            generate_response(messages: List[Dict]) Tuple[str, Any, str]
          }
          class Google {
            get_model_for(model_spec: backends.ModelSpec) backends.Model
          }
          class GoogleModel {
            model
            download_image(image_url)
            encode_images(images)
            encode_messages(messages)
            generate_response(messages: List[Dict]) Tuple[str, Any, str]
            upload_file(file_path, mime_type)
          }
          class HuggingfaceLocal {
            get_model_for(model_spec: backends.ModelSpec) backends.Model
          }
          class HuggingfaceLocalModel {
            config
            context_size : int
            device : str
            model
            tokenizer : PreTrainedTokenizerFast
            generate_response(messages: List[Dict], return_full_text: bool, log_messages: bool) Tuple[Any, Any, str]
          }
          class HuggingfaceMultimodal {
            get_model_for(model_spec: backends.ModelSpec) backends.Model
          }
          class HuggingfaceMultimodalModel {
            context_size
            cull
            device : str
            idefics
            model_name
            model_type
            multimodal_model
            padding
            processor
            split_prefix
            supports_multiple_images
            template
            generate_response(messages: List[Dict]) Tuple[Any, Any, str]
          }
          class HumanModel {
            generate_response(messages: List[Dict])* Tuple[Any, Any, str]
          }
          class LlamaCPPLocal {
            get_model_for(model_spec: backends.ModelSpec) backends.Model
          }
          class LlamaCPPLocalModel {
            chat_formatter
            context_size
            model
            generate_response(messages: List[Dict], return_full_text: bool) Tuple[Any, Any, str]
          }
          class Mistral {
            client
            get_model_for(model_spec: backends.ModelSpec) backends.Model
            list_models()
          }
          class MistralModel {
            client
            generate_response(messages: List[Dict]) Tuple[str, Any, str]
          }
          class Model {
            model_spec
            generate_response(messages: List[Dict])* Tuple[Any, Any, str]
            get_gen_arg(arg_name)
            get_max_tokens()
            get_name() str
            get_temperature()
            set_gen_arg(arg_name, arg_value)
            set_gen_args()
          }
          class ModelSpec {
            HUMAN_SPECS : list
            PROGRAMMATIC_SPECS : list
            from_dict(spec: Dict)
            from_name(model_name: str)
            has_attr(attribute)
            has_backend()
            has_temperature()
            is_human()
            is_programmatic()
            unify(other: 'ModelSpec') 'ModelSpec'
          }
          class OpenAI {
            client : OpenAI
            get_model_for(model_spec: backends.ModelSpec) backends.Model
            list_models()
          }
          class OpenAIModel {
            client : OpenAI
            encode_image(image_path)
            encode_messages(messages)
            generate_response(messages: List[Dict]) Tuple[str, Any, str]
          }
          class Player {
            descriptor : Optional[str]
            model : Model
            get_description() str
          }
          class TaskBot {
            task_id
            join_task_room()
            move_divider(room_id, chat_area, task_area)
          }
          CustomResponseModel --|> Model
          HumanModel --|> Model
          DialogueGameMaster --|> GameMaster
          GameBenchmark --|> GameResourceLocator
          GameInstanceGenerator --|> GameResourceLocator
          GameMaster --|> GameRecorder
          GameRecorder --|> GameResourceLocator
          GameScorer --|> GameResourceLocator
          APIChatBot --|> GenericChatbot
          GenericChatbot --|> TaskBot
          TaskBot --|> Bot
          ModelSpec --o Model : model_spec
  
       </div>
  </body>
</html>
